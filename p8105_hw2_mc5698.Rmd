---
title: "p8105_hw2_mc5698.Rmd"
output: github_document
  
date: "2024-09-27"
---

#Question 1
```{r}
#loading necessary packages
library(tidyverse)
library(readxl)
```

```{r}
#clean the dataset
nyc_t = 
  read_csv(
    "/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10,route11, entry, vending, entrance_type, ada) |> 
  mutate(
    entry = ifelse(entry == "YES", TRUE, FALSE))
```

The dataset contains `r names(nyc_t)`. For the data cleaning, I removed unnecessary columns and convert the entry variable from character to a logical variable by using `case_match` function. The dimension of the resulting dataset is `r dim(nyc_t)`. These data are mostly tidy but we could pivot different route columns into one variable. 
```{r}
distinct_stations=
  nyc_t|>
  distinct(station_name,line)
nrow(distinct_stations)
```
```{r}
ada_stations=
  nyc_t|>
  filter(ada==TRUE)|>
  distinct(station_name, line)

nrow(ada_stations)
```
```{r}
no_vending=
  nyc_t |>
  filter(vending == "NO") |>
  pull(entry)
 
proportion_entry= mean(no_vending)
proportion_entry
```

There are 465 distinct stations. 84 stations are ADA compliant.The proportion of station entrances/exits without vending allow entrance is 0.3770492. 

```{r}
transfrom_ent=
  nyc_t |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route")

A_stations=
  transfrom_ent|>
  filter(route == "A") |> 
  distinct(station_name, line) 
  

nyc_t |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  distinct(station_name, line)



```
There are 60 distinct stations serve the A train and 17 stations serve the A train and ADA compliant.

#Question 2
```{r}
#clean the datasets
mr_trash_wheel = 
  readxl::read_excel("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/202409TrashWheelCollectionData.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N655") |> 
  filter(!is.na(Dumpster)) |> 
  mutate(Sports_Balls = as.integer(round(`Sports Balls`)),
         Year = as.character(Year),
         Trash_Wheel = "Mr. Trash Wheel")

professor_trash_wheel = 
  readxl::read_excel("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/202409TrashWheelCollectionData.xlsx", sheet = "Professor Trash Wheel", skip = 1) |> 
  filter(!is.na(Dumpster)) |> 
  mutate(Year = as.character(Year),
         Trash_Wheel = "Professor Trash Wheel")

gwynnda_trash_wheel = 
  readxl::read_excel("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/202409TrashWheelCollectionData.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1) |> 
  filter(!is.na(Dumpster)) |> 
  mutate(Year = as.character(Year),
         Trash_Wheel = "Gwynnda Trash Wheel")

combined_data = 
  bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda_trash_wheel)
combined_data
```

By reading and cleaning the datasets, I combined the three datasets from `Mr. Trash Wheel`, `Professor Trash Wheel` and `Gwynnda Trash Wheel`. There are `r nrow(combined_data)` observations in the combined dataset. This dataset includes key variables such as`Dumpster`, which shows the the number of dumpster filled by trash, and `Cigarette Butts` which means the number of cigarette they collected. It also includes the specific time of the trash such as `Year`, `Date`, `Month` and `Trash_Wheel` indicates different trash types correspond to the different trash wheel. Moreover, it provides the detailed volumn and types for each trash wheel. 

```{r}
tw_professor = 
  combined_data |> 
  filter(Trash_Wheel == "Professor Trash Wheel") |> 
  summarise(total_weight = sum(`Weight (tons)`, na.rm = TRUE))
```

```{r}
cb_gwynnda_june2022 = 
  combined_data |> 
  filter(Trash_Wheel == "Gwynnda Trash Wheel", Year == "2022", Month == "June") |> 
  summarise(total_cig_butts = sum(`Cigarette Butts`, na.rm = TRUE))
```
The total weight of trash collected by Professor Trash Wheel wad  `r pull(tw_professor)`. The total number of cigarette butts collected by Gwynnda in June of 2022 was `r pull(cb_gwynnda_june2022)`.

#Question 3
```{r warning=FALSE, message=FALSE}
#read and clean the datasets
bakers_data = 
  read_csv("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/gbb_datasets/bakers.csv") |>
  janitor::clean_names() %>%
  mutate(source = "bakers")


bakes_data = 
  read_csv("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/gbb_datasets/bakes.csv") |>
  janitor::clean_names() %>%
  mutate(source = "bakes")

results_data = 
  read_csv("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/gbb_datasets/results.csv", skip = 2) |>
  janitor::clean_names()

viewers_data = 
  read_csv("/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/gbb_datasets/viewers.csv") |>
  janitor::clean_names() 
```

```{r}
#rename the column names
bakers_data <- bakers_data %>%
  rename(baker = baker_name)

# Extract the first word 
bakers_data$baker <- sapply(strsplit(bakers_data$baker, " "), function(x) x[1])
```

```{r}
#check for completeness and correctness across datasets 
missing_bakers <- anti_join(bakes_data, bakers_data, by = c("baker"))

missing_bakes <- anti_join(results_data, bakes_data, by = c("series", "episode", "baker"))

```

```{r}

# merge the datasets
final_dataset =
  merge(bakers_data, bakes_data, all= TRUE)
  
final_dataset =
  merge(final_dataset,results_data, all= TRUE)

```

```{r}
write_csv(final_dataset, "/Users/nicolechen/Downloads/p8105_hw2_mc5698/dataset/gbb_datasets/final_dataset.csv")
```
For this project, I cleaned and organized data from `bakers.csv`, `bakes.csv`, `results.csv`, and `viewers.csv.` Firstly, I renamed the `Baker Name` column to `Baker` in the `bakers.csv` file to make it easier to match with other datasets. Then, I noticed that the baker names had different formats, so I used a function to convert all the names to lowercase and remove extra spaces.I also transformed the `Series` and `Episode`· columns to numeric for easier merging. For `viewers.csv`, I reshaped the data from wide to long format to align with the other files.
After checking for missing bakers between the files, I merged the datasets step by step: first, results with `bakers`, then with `bakes`, and finally with `viewers.` I organized the data by series and episode to make it more readable. The final dataset contains all relevant information, including bakers' details, results, and viewership.

```{r}
#Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10
star_baker =
  final_dataset %>%
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode , baker, result) %>%
  arrange(series, episode)
star_baker
```
From the table, I found that some people such as Richard become star bakers or winners in multiple episodes, which might make their overall success predictable.
```{r}
#import, clean, tidy, and organize the viewership data
head(viewers_data, 10)

season_1 = mean(viewers_data$series_1, na.rm = TRUE)

season_5 = mean(viewers_data$series_5, na.rm = TRUE)

```
The average viewership in Season 1 is  `r season_1`, and the average viewership in Season 5 is approximately `r season_5`, showing the growth in the show’s popularity.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

